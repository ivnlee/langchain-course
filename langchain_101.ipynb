{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xunr69_vDKMbYaNqujst0h_Q5PLRgVHv",
      "authorship_tag": "ABX9TyMxxENiFki0BF7BJWEoDouf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivnlee/langchain-course/blob/main/langchain_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain & Vector Databases**\n",
        "\n",
        "### **Module 1: From Zero to Hero**"
      ],
      "metadata": {
        "id": "0zabqzFVrT5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Xt9ceGrTH4"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.208 deeplake openai tiktoken\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('api_key.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IZeZcfWr_QE",
        "outputId": "9cce6585-6c27-4850-be1b-f8ca3b86c951"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The LLMs**\n",
        "\n"
      ],
      "metadata": {
        "id": "cFN8X59a_ZjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The fundamental component of LangChain involves invoking an LLM with a specific input.\n",
        "\n",
        "#### To accomplish this, we will first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "tKf3iy9g_wT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "lWj8VIGluHVd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)"
      ],
      "metadata": {
        "id": "qvw1N5dkuQ3U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PCUJnwruSuf",
        "outputId": "aabb4f78-54c9-4417-c192-2658f8b6fc34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Jogging: Start with a 15-minute jog, increasing gradually to a 30-minute jog 3-4 times a week.\n",
            "\n",
            "2. Cycling: Cycle for 30 minutes 3-4 times a week, gradually increasing the duration as you get more fit.\n",
            "\n",
            "3. Swimming: Swim laps for 30 minutes, 3-4 times a week. Start with a light level of intensity and then increase as you get more fit.\n",
            "\n",
            "4. Hill running: Run up & down hills for 15 minutes 2-3 times a week, increasing the intensity and duration as you get more fit.\n",
            "\n",
            "5. Interval training: Aside from your regular cardio routine, do 2-3 days of interval training. Start with a 10-minute warm up, then do 1 minute of sprinting followed by 1 minute of walking. Repeat for a total of 20 minutes.\n",
            "\n",
            "6. Strength training: Do strength training exercises at least once a week to improve your overall strength, which will help you with your cardiovascular endurance. \n",
            "\n",
            "7. Outdoor activities: Incorporate more outdoor activities into your routine. Go rock climbing, take a hike, or join a local sports team. Outdoor activities can be a great way to get your heart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Chains**"
      ],
      "metadata": {
        "id": "QninESMrABZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence."
      ],
      "metadata": {
        "id": "4y9_DlswAbrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most commonly used type of chain is the `LLMChain`, which consists of a `PromptTemplate`, a model (either an LLM or a ChatModel), and an optional output parser.\n",
        "\n",
        "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's `LLMChain`, `PromptTemplate`, and `OpenAI` classes, we can easily define our prompt, set the input variables, and generate creative outputs."
      ],
      "metadata": {
        "id": "Tb2CFRFiBSao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "DODYI-2BAjJk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"eco-friendly water bottles\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiRnzfn8Bred",
        "outputId": "4c5f9253-3c58-46a5-dcbe-51a5ea0bfd2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EcoPure Bottles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Memory**"
      ],
      "metadata": {
        "id": "RH1JQSh2DDjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses."
      ],
      "metadata": {
        "id": "dyCdvc_3Duhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "BIbzE64pDF-c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaWOgUiBECwa",
        "outputId": "2510860a-7cf0-43cd-df49-e4d9a1bd71d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\", additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=\" I'm not currently able to help with data analysis, but I'm always learning and expanding my capabilities. I'm sure I'll be able to help you with data analysis in the future.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cjROogPHE8Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deep Lake VectorStore**"
      ],
      "metadata": {
        "id": "pCR-2GMZE-u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deeplake"
      ],
      "metadata": {
        "id": "8PFBKc4UFEQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "ZVcFjwlJHOhF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the LLM and embeddings models\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create our documents\n",
        "texts = [\n",
        "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"ivanlee\"\n",
        "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
        "\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEQGN_A9HRRA",
        "outputId": "a7754efe-4913-4d4a-dfe2-5a686f02a325"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n",
            "The dataset is private so make sure you are logged in!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://ivanlee/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            "   text       text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fa0d48b6-1a21-11ee-bb9d-0242ac1c000c',\n",
              " 'fa0d4ad2-1a21-11ee-bb9d-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a `RetrievalQA` chain:"
      ],
      "metadata": {
        "id": "Lw2jIRi_ItbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "UyVhEVTMIy1r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's create an agent that uses the `RetrievalQA` chain as a tool:"
      ],
      "metadata": {
        "id": "m7nwT05YI6Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "9NRBtzkOJCKU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can use the agent to ask a question:"
      ],
      "metadata": {
        "id": "hPYHMiAQKaHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kRrALcxKa-v",
        "outputId": "a75a9f39-b026-4443-c89d-6557796b013d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born.\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Napoleone born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Napoleon Bonaparte was born on 15 August 1769.\n"
          ]
        }
      ]
    }
  ]
}